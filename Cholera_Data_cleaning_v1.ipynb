{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shfarhaan/ml-notebooks/blob/main/Cholera_Data_cleaning_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyxRKLE17BqN"
      },
      "outputs": [],
      "source": [
        "#!pip install torch transformers\n",
        "#!pip install scispacy\n",
        "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
        "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz\n",
        "#!pip install transformers torch pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from google.colab import drive\n",
        "import scispacy\n",
        "import spacy\n",
        "import string\n"
      ],
      "metadata": {
        "id": "MgJh5lyo_C7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13887e90-2735-4d7c-a28b-2de7e7f41354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load disease_data csv file\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/Omdena/NLP in drug prediction/all_merged_datasets_v2.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT_RAms_Fv0q",
        "outputId": "f09da355-1324-4f56-affa-a569aec78eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Data cleaning\n",
        "\n",
        "# Select all text columns\n",
        "text_columns = [\"Disease name\", \"Symptoms\", \"Source\", \"Treatment\", \"Diagnosis\", \"Drugs name\"]\n",
        "df[text_columns] = df[text_columns].astype(str).fillna('')\n",
        "\n",
        "# 1) Clean Text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)  # Remove punctuationw\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "df[text_columns] = df[text_columns].applymap(clean_text)\n",
        "\n",
        "\n",
        "\n",
        "### Delete duplicates according to Disease name, symptoms and source\n",
        "# Group by 'Disease name' and 'Source', selecting the row with the most abundant Symptoms data while keeping all columns\n",
        "df = df.groupby(['Disease name', 'Source'], group_keys=False).apply(\n",
        "    lambda group: group.loc[group['Symptoms'].str.len().idxmax()] if group['Symptoms'].notna().any() else group.iloc[0]\n",
        ").reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D-YfF765S9J",
        "outputId": "427591da-0b21-455c-aa20-66816b781cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-a5a24394d298>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df[text_columns] = df[text_columns].applymap(clean_text)\n",
            "<ipython-input-31-a5a24394d298>:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(['Disease name', 'Source'], group_keys=False).apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Extract Medical Entities using SciSpaCy\n",
        "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
        "# Function to extract specific medical entities\n",
        "def extract_entities(text, entity_type):\n",
        "    if pd.isna(text):  # Handle missing values\n",
        "        return \"\"\n",
        "\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ == entity_type]\n",
        "    return \", \".join(entities) if entities else \"\"\n",
        "\n",
        "# Apply entity extraction to each column with the specified entity type\n",
        "df[\"Symptoms_SciSpaCy\"] = df[\"Symptoms\"].apply(lambda x: extract_entities(x, \"DISEASE\"))\n",
        "df[\"Diagnosis_SciSpaCy\"] = df[\"Diagnosis\"].apply(lambda x: extract_entities(x, \"DISEASE\"))\n",
        "df[\"Drugs_SciSpaCy\"] = df[\"Drugs name\"].apply(lambda x: extract_entities(x, \"CHEMICAL\"))\n",
        "df[\"Treatment_SciSpaCy\"] = df[\"Treatment\"].apply(lambda x: extract_entities(x, \"TREATMENT\"))\n"
      ],
      "metadata": {
        "id": "kHUCwVur6t2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Extract Medical Entities using bioBert\n",
        "# âœ… Use a fine-tuned BioBERT model trained on the BC5CDR dataset (for drug NER)\n",
        "#model_name = \"d4data/biobert_cased_ner_bc5cdr\"\n",
        "model_name = \"alvaroalon2/biobert_chemical_ner\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a Named Entity Recognition (NER) pipeline\n",
        "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "# Function to extract named entities\n",
        "def extract_entities(text):\n",
        "    if isinstance(text, str):  # Ensure text is a string\n",
        "        ner_results = nlp_ner(text)\n",
        "        entities = [(entity['word'], entity['entity_group'], entity['score']) for entity in ner_results]\n",
        "        return entities\n",
        "    return None\n",
        "\n",
        "# Apply function to each row in 'Treatment' column to get drug name\n",
        "df[\"extracted_Treatment_BioBERT\"] = df[\"Treatment\"].apply(extract_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y26FkKl5FS-d",
        "outputId": "4fb60250-36a3-4474-ae1e-90b5b2cf9691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/gdrive/MyDrive/Omdena/NLP in drug prediction/all_merged_datasets_v3.csv', index = False)"
      ],
      "metadata": {
        "id": "eg_LfS899GSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code ends here"
      ],
      "metadata": {
        "id": "EYci2NTSTnPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Extract Medical Entities using bioBert\n",
        "\n",
        "# Load BioBERT NER models\n",
        "ner_disease = pipeline(\"ner\", model=\"dmis-lab/biobert-base-cased-v1.1\", grouped_entities=True)  # For Symptoms & Diagnosis\n",
        "ner_drugs = pipeline(\"ner\", model=\"dmis-lab/biobert-base-cased-v1.1\", grouped_entities=True)  # For Drugs\n",
        "ner_treatment = pipeline(\"ner\", model=\"michiyasunaga/BioLinkBERT-base\", grouped_entities=True)  # For Treatments\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer for truncation\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "def extract_entities(text, model, target_types):\n",
        "    \"\"\"\n",
        "    Extracts named entities from text using a specified BioBERT model.\n",
        "\n",
        "    Parameters:\n",
        "    - text (str): The input text.\n",
        "    - model (transformers.pipeline): The NER model to use.\n",
        "    - target_types (list): The entity labels to extract.\n",
        "\n",
        "    Returns:\n",
        "    - str: A comma-separated string of extracted entities.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    # âœ… Truncate text to first 512 tokens to prevent errors\n",
        "    tokenized_text = tokenizer(text, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    truncated_text = tokenizer.decode(tokenized_text[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "    # Process truncated text\n",
        "    entities = model(truncated_text)\n",
        "    filtered_entities = [ent['word'] for ent in entities if ent['entity_group'] in target_types]\n",
        "\n",
        "    return \", \".join(filtered_entities) if filtered_entities else \"\"\n",
        "\n",
        "\n",
        "\n",
        "# Ensure no NaN values\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Apply models to the correct columns\n",
        "df[\"Symptoms_Extracted\"] = df[\"Symptoms\"].apply(lambda x: extract_entities(x, ner_disease, [\"DISEASE\"]))\n",
        "df[\"Diagnosis_Extracted\"] = df[\"Diagnosis\"].apply(lambda x: extract_entities(x, ner_disease, [\"DISEASE\"]))\n",
        "df[\"Drugs_Extracted\"] = df[\"Drugs name\"].apply(lambda x: extract_entities(x, ner_drugs, [\"CHEMICAL\"]))\n",
        "df[\"Treatment_Extracted\"] = df[\"Treatment\"].apply(lambda x: extract_entities(x, ner_treatment, [\"TREATMENT\"]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Mlr7C5FTnnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PZNbcq18fi1O"
      }
    }
  ]
}